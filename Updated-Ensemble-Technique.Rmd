---
title: "Ensemble Techniques"
author: "Bishal Neupane, Saugat Gyawali"
date: "10/08/2022"
output: pdf_document
always_allow_html: true
---


### Source: 
 https://www.kaggle.com/code/abhpasha/logistic-regression-predicting-rain-in-australia

### Importing data and taking only first 15k because the data is two large more than 100k
```{r}
df <- read.csv("weatherAUS.csv", header = TRUE)


```
```{r}
head(df)
```
#There are alot of column so removing columns with non numeric values.
```{r}
df$Date<- NULL
df$WindGustDir<-NULL
df$WindGustDir <-NULL
df$WindDir3pm <- NULL
df$WindDir3pm <-NULL
df$Location <-NULL
df$Sunshine <-NULL
df$RainToday <- NULL
df$WindDir9am <-NULL
df$Evaporation <-NULL 
```

### Structure of Data Frame

```{r}
str(df)
```

## Data Exploration

### Names of Column
```{r}
names(df)

```
### Importing Package and  using it to Change to factor
```{r}
#install.packages("dplyr")
library(dplyr)
df <- mutate_if(df, is.character, as.factor)

```

### Dimensions of df
```{r}
dim(df)
```


```{r}
str(df)
```


### Statistics Summary of Each column
```{r}
summary(df)
```

### Exploring Missing values
```{r}
sum(is.na(df))
```
### Removing the row with target value NA
```{r}
df <- subset(df,RainTomorrow  != "NA")

```





### Dimension after removing rows with NA as Rain Tomorrow
```{r}
dim(df)
```



```{r}
str(df)
```



### Replacing NA's with mean of a column

```{r}

#install.packages('tidyr')
for(i in 1:ncol(df)){
  df[is.na(df[,i]), i] <- mean(df[,i], na.rm = TRUE)
}
```

### Summary after replacing NA's with mean
```{r}
summary(df)
```









## Data Visualization
```{r}
par(mfrow=c(1,6))
plot(df$RainTomorrow, df$MinTemp, data=df, main="MinTemp",
varwidth=TRUE)
plot(df$RainTomorrow, df$MaxTemp, data=df, main="MaxTemp", varwidth=TRUE)
plot(df$RainTomorrow, df$Rainfall, data=df, main="Rainfall", varwidth=TRUE)
plot(df$RainTomorrow, df$Evaporation, data=df, main="Evaporation", varwidth=TRUE)
plot(df$RainTomorrow, df$Sunshine, data=df, main="Sunshine", varwidth=TRUE)

plot(df$RainTomorrow, df$WindGustSpeed, data=df, main="windGustSpeed",
varwidth=TRUE)


```
```{r}
boxplot(df, col = rainbow(ncol(df)))
```
```{r}
par(mfrow=c(3,5))
cdplot(df$RainTomorrow~df$Humidity3pm)
cdplot(df$RainTomorrow~df$WindSpeed3pm)
cdplot(df$RainTomorrow~df$MinTemp)
cdplot(df$RainTomorrow~df$MaxTemp)
cdplot(df$RainTomorrow~df$Rainfall)
cdplot(df$RainTomorrow~df$WindGustSpeed)
cdplot(df$RainTomorrow~df$WindSpeed9am)
cdplot(df$RainTomorrow~df$Humidity9am)
cdplot(df$RainTomorrow~df$Pressure9am)
cdplot(df$RainTomorrow~df$Temp3pm)
cdplot(df$RainTomorrow~df$Temp9am)
cdplot(df$RainTomorrow~df$Cloud3pm)
cdplot(df$RainTomorrow~df$Cloud9am)


```

##Spliting into train and test set

```{r}
set.seed(1234)
i <- sample(1:nrow(df), 0.80*nrow(df), replace=FALSE)
train <-df[i,]
test <- df[-i,]
testLabel <- test$RainTomorrow

trainLabel <- train$RainTomorrow

```

## Decision Tree
```{r}
library(tree)
tree_weather <- tree(RainTomorrow~.,data = train)
plot(tree_weather)
text(tree_weather, cex= 0.5, pretty=0)
```
### Prediction, Confusion Matrix and Statistics
```{r}
prediction <- predict(tree_weather, newdata = test, type = "class")
table(prediction, testLabel)
```
```{r}
levels(prediction) <- list("1" = "No", "2" = "Yes")
levels(testLabel) <- list("1" = "No", "2" = "Yes")
library(caret)
confusionMatrix(as.factor(prediction),as.factor(testLabel))
```


```{r}
par(mfrow=c(1,2))
cv_tree <- cv.tree(tree_weather)
plot(cv_tree$size, cv_tree$dev, type="b")
tree_pruned <- prune.tree(tree_weather, best=5)
plot(tree_pruned)
text(tree_pruned, pretty=0)

```
```{r}

prediction1 <- predict(tree_weather, newdata = test, type = "class")
table(prediction1, testLabel)
```
```{r}
levels(prediction) <- list("1" = "No", "2" = "Yes")
levels(testLabel) <- list("1" = "No", "2" = "Yes")
library(caret)
confusionMatrix(as.factor(prediction),as.factor(testLabel))
```
## Random Forest 
```{r}
#install.packages("randomForest")
library(randomForest)
set.seed(1234)
randomf <- randomForest(train$RainTomorrow~., data = train, importance = TRUE)

```
### Recution and confusion Matrix

```{r}
pred <- predict(randomf, newdata = test, type = "response")
levels(pred) <- list("1" = "No", "2" = "Yes")
levels(testLabel) <- list("1" = "No", "2" = "Yes")
library(caret)
confusionMatrix(as.factor(pred),as.factor(testLabel))
```



### Boosting
```{r}
#install.packages('adabag')
library(adabag)
ada1 <- boosting(RainTomorrow~., data = train, boos = TRUE, mfinal =15, coeflearn = "Breiman")
summary(ada1)
```

### Result and Confusion Matrix
```{r}
pred <- predict(ada1, newdata = test, type = "response")

accuracy <- mean(pred$class==test$RainTomorrow)
print(paste("accuracy is ", accuracy))
```

## XGBOOST

```{r}
#install.packages('xgboost')
library(xgboost)

#levels(trainLabel) <- list("0" = "No", "1" = "Yes")

model <- xgboost(data=data.matrix(train), label=trainLabel,nrounds=100)
summary(model)
```

```{r}
levels(test$RainTomorrow) <- list("1" = "No", "2" = "Yes")
probs <- predict(model, data.matrix(test))
pred <- ifelse(probs>0.5, 1, 0)
levels(pred) <- list("1" = "No", "2" = "Yes")
levels(testLabel) <- list("1" = "No", "2" = "Yes")
library(caret)
confusionMatrix(as.factor(pred),as.factor(testLabel))
```

### bgboost visulation
```{r}
#install.packages(DiagrammeR)
library(DiagrammeR)
xgb.plot.tree(model=model, trees =1:3)

```

## Analysis Based on Run Time and Metrics:
I used Decision Tree, Random forest, XGBoost and boosting to perform the classification. Their analysis based on the run time and metrics can be done in the following ways:

The accuracy of decision tree was about 82 percent initially and later after pruning tree, there was not significant increase in accuracy. But the accuracy of the random forest was about 85 percent. According to the accuracy, random forest outperforms decision tree which is technically true. It is because decision tree uses the concept of feature importance and make prediction by making one tree but random forest selects the features randomly and make a forest of many decision tree. It will finally combine the result of all the decision trees in the forest and generialize the result more accuratly. But the run time of the random forest is more than the run time of decision tree because decision tree creates just a tree and random forest creates many trees and combine result. Accuracy of XGboost is about 77 percent while the accuracy of random forest and decision tree is 85 and 82 percent respectively. This is true because the data set I have is multiclass classification and a lot of data was missing initally which might have increased the noise in the data. But for the Xgboot, it will have great accuracy if the data was unbalanced. The run time of XGboost in my experiment was less than the run time of random forest. It is technically true because random forest created diffeent decision tree and combine the result later but XGboost creates the result and pass the result to another which make it efficient. The accuracy of adaboosting was about 83 percent which was little higher than decision tree and less than random forest. This is technically true because decision tree uses only on tree while ada use decision lump node and two children for decision. But random forest uses many decision trees and get the result from all at last. So, accuracy of random forest is definately high. But the time of ada is comperatively less than random forset. Random tree creates all the trees and get result while ada uses decision lumps for decision which is basically node and two children. 

